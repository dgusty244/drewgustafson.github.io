---
layout: default
title: "Redesigning High Watermark Logic at WTW"
---
<article class="project-article">
  <h1>Redesigning High Watermark Logic at WTW</h1>
  <p>When I joined WTW, I inherited an incremental data load framework responsible for populating over 20—possibly over 50—tables in an Azure-based data warehouse. These tables were grouped into what the team called high watermark groups, and their processing logic depended on a pipeline start time.</p>
  <p>At the beginning of each run, the pipeline would set the high watermark to the current timestamp, then look back a fixed window—typically three hours—using that value in its filters. However, if a job failed, the watermark had already advanced by the next run, and the data might be missed. To compensate, the team ran frequent manual and automated backfills, including a regular three-month backfill every Sunday.</p>
  <p>Additionally, the ETL timestamps in the source OLTP database were not always reliable. One particularly fragile pattern used was:</p>
  <pre><code class="language-sql">
SELECT * 
FROM table_a 
WHERE EXISTS (
    SELECT 1 
    FROM table_b 
    WHERE table_a.col = table_b.col 
    AND table_b.timestamp > highWatermark
)
  </code></pre>
  <p>This indirect filtering often led to inconsistent results. The Azure data warehouse being loaded wasn't yet in use for reporting (which was still handled by an on-prem SSIS pipeline), so fixing this framework hadn't been a priority.</p>
  
  <h2>Redesigning the Watermark Strategy</h2>
  <p>When I was given the choice to use the existing framework or start fresh, I chose a hybrid approach. I reused parts of the existing pipeline but replaced the watermark logic entirely.</p>
  
  <div class="diagram-container" style="text-align: center; margin: 2rem 0;">
    <img src="images/watermark-redesign-architecture.png" alt="High Watermark Logic Redesign Architecture" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
    <p style="font-style: italic; color: #666; margin-top: 0.5rem; font-size: 0.9rem;">Architecture comparison showing the transition from pipeline-based to data-driven watermarks</p>
  </div>
  
  <p>Instead of using the pipeline's start time as the high watermark—which is unrelated to the data itself—I began storing the <strong>maximum value of the column actually used to filter the data</strong> (such as an <code>updated_at</code> timestamp). This ensured that the watermark was consistent with the data's own change history.</p>
  <blockquote>
    <strong>Why this matters:</strong>  
    Using pipeline start time as a watermark risks skipping records if a job fails or runs late. Using the maximum value from the actual data ensures correctness, repeatability, and easier recovery from failure.
  </blockquote>
  <p>I also introduced zone-specific high watermarks. Each table tracked its watermark independently in the bronze, silver, and gold zones of our lakehouse. For example, if a job succeeded in loading bronze but failed in silver, it wouldn't reprocess the entire dataset—just the delta from the previous zone's high watermark.</p>
  <p>Since our external SQL warehouse (used for business reporting) only received the final gold output, I added a dedicated high watermark to track what had been merged there. This prevented duplicate inserts and guaranteed completeness across all zones.</p>
  <h2>Results</h2>
  <p>After implementing these changes, our pipeline runs became far more stable and predictable. Failed jobs could resume cleanly from the correct point. Backfills were no longer required as frequently. While occasional backfills remained a reality—often due to source-side issues—I focused on root cause elimination instead of just symptom recovery.</p>
  <p>This project taught me that even systems that appear to be working may harbor hidden fragility. By questioning inherited assumptions and rethinking the high watermark strategy, I delivered reliability improvements without a full rewrite.</p>
</article>
