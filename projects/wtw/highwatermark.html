---
layout: default
title: Redesigning High Watermark Logic at WTW | Drew Gustafson
---

<div class="container">
  <article class="project-article">
    <h1>Redesigning High Watermark Logic at WTW</h1>

    <p>When I joined WTW, I inherited an incremental data load framework responsible for populating dozens of tables in an Azure-based data warehouse. These tables were grouped into what the team called high watermark groups, and their processing logic depended on a pipeline start time. Notably, there was no table-level control of high watermarks beyond the bronze zone, and pre-advancing of watermarks occurred only for zones beyond bronze. The source-to-bronze zone processing was actually implemented correctly.</p>

    <p>At the beginning of each run, the pipeline would set the high watermark to the current timestamp, then look back a fixed window, typically three hours, using that value in its filters. However, if a job failed, the watermark had already advanced by the next run and the data might be missed. To compensate, the team ran frequent manual and automated backfills, including a regular three-month backfill every Sunday.</p>

    <p>Additionally, the ETL timestamps in the source OLTP database were not always reliable. One particularly fragile pattern used was:</p>

    <pre><code class="language-sql">
SELECT * 
FROM table_a 
WHERE EXISTS (
    SELECT 1 
    FROM table_b 
    WHERE table_a.col = table_b.col 
    AND table_b.timestamp > highWatermark
)
    </code></pre>

    <p>This indirect filtering often led to inconsistent results. The Azure data warehouse being loaded wasn't yet in use for reporting (which was still handled by an on-prem SSIS pipeline), so fixing this framework hadn't been a priority.</p>

    <h2>Redesigning the Watermark Strategy</h2>

    <p>This redesign was specifically for new projects; reworking the in-place incremental loads was not a priority, especially since the Azure data warehouse wasn't used for live reporting yet. I initially chose a hybrid approach to the existing framework, then eventually built an entirely new orchestration framework â€” <a href="https://dgusty244.github.io/drewgustafson.github.io/projects/wtw/EtlFramework.html" target="_blank" rel="noopener noreferrer">read more about it here</a>.</p>

    <div class="diagram-container" style="text-align: center; margin: 2rem 0;">
      <img src="{{ site.baseurl }}/images/watermark-redesign-architecture.png" alt="High Watermark Logic Redesign Architecture" style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 8px; box-shadow: 0 4px 8px rgba(0,0,0,0.1);">
      <p style="font-style: italic; color: #666; margin-top: 0.5rem; font-size: 0.9rem;">Architecture comparison showing the transition from pipeline-based to data-driven watermarks</p>
    </div>

    <p>Instead of using the pipeline's start time as the high watermark, which is unrelated to the data itself, I began storing the <strong>maximum value of the column actually used to filter the data</strong> (such as an <code>updated_at</code> timestamp). This ensured that the watermark was consistent with the data's own change history.</p>

    <blockquote>
      <strong>Why this matters:</strong>  
      Using pipeline start time as a watermark risks skipping records if a job fails or runs late. Using the maximum value from the actual data ensures correctness, repeatability, and easier recovery from failure.
    </blockquote>

    <p>I also introduced zone-specific high watermarks. Each table tracked its watermark independently in the bronze, silver, and gold zones of our lakehouse. For example, if a job succeeded in loading bronze but failed in silver, it wouldn't reprocess the entire dataset, just the delta from the previous zone's high watermark.</p>

    <p>Since our external SQL warehouse, used for business reporting, only received
