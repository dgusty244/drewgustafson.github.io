---
layout: default
title: ETL Framework Redesign at WTW | Drew Gustafson
---

<div class="container">
  <h1>ETL Framework Redesign at WTW</h1>

  <div class="section">
    <p>
      At WTW, I led the end-to-end redesign and implementation of a metadata-driven ETL framework, replacing a legacy system. The new system is robust, flexible, and still actively used across the organization. It supports any data source compatible with Azure Databricks and Azure Data Factory (ADF), and emphasizes modularity, reuse, and observability.
    </p>
  </div>

  <div class="section tech-stack">
    <h2>Tech Stack</h2>
    <ul>
      <li><strong>Orchestration:</strong> Azure Data Factory</li>
      <li><strong>Transformations:</strong> Azure Databricks, PySpark</li>
      <li><strong>Metadata System:</strong> Azure SQL DB</li>
    </ul>
  </div>

  <div class="section">
    <h2>Metadata Entity Relationship Diagram</h2>
    <p><em>Field names have been modified for confidentiality while preserving core business logic.</em></p>
    <!-- Leading slash here is important to load from root folder /images/ERD.png -->
    <img src="{{ site.baseurl }}/images/ERD.png" alt="Metadata Entity Relationship Diagram" />

  </div>

  <div class="section">
    <h2>Metadata-Driven Architecture</h2>
    <p>
      The core design principle was to shift from static, monolithic pipelines to dynamic, metadata-controlled workflows. Instead of building a new pipeline for each request, I designed workflows as collections of modular jobs, where each job is executed by a reusable pipeline. Workflow structure, job dependencies, parameters, and schedules are all stored in metadata tables. This approach dramatically reduced code duplication and allowed teams to scale workflow creation with minimal new development.
    </p>
    <p>
      Example: When a new source—a file share on the company network—needed integration, I only created a new ingestion pipeline for that source. All transformation and warehouse loading logic reused existing pipelines through parameter configuration.
    </p>
  </div>

  <div class="section">
    <h2>Dynamic Scheduling</h2>
    <p>
      I implemented a polling-based dynamic scheduler to replace ADF’s rigid, trigger-heavy system. Scheduling information is stored as JSON in the metadata table and parsed via a SQL view. A <code>minutesSinceScheduled</code> column identifies workflows ready to run (e.g., between 0–4 minutes late), which are then picked up by a central driver pipeline. This made it easier to enable/disable workflows using a simple <code>isActive</code> flag and avoided the silent failures previously caused by ADF triggers being turned off or misconfigured.
    </p>
  </div>

  <div class="section">
    <h2>Job Dependency Management with DAG Logic</h2>
    <p>
      The legacy system scheduled jobs using hard-coded job order numbers. I replaced this with a directed acyclic graph (DAG) dependency system using a <code>dependsOnJobId</code> column in metadata.
    </p>
    <p>
      A recursive CTE in SQL unrolled this dependency tree into a clean one-row-per-dependency format. This view is used to:
    </p>
    <ul>
      <li>Identify which jobs to run at the start of a workflow (no upstream dependencies).</li>
      <li>Dynamically queue successor jobs once predecessors finish.</li>
    </ul>
    <p>
      This improved transparency, reduced human error, and made workflows much easier to scale or modify.
    </p>
  </div>

  <div class="section">
    <h2>Notebook & Pipeline Standardization</h2>
    <p>
      Previously, developers created a new notebook for each new request, even if the logic was repetitive. I introduced a parameter-driven notebook template system, where common transformations could be reused by simply passing different parameters. Similarly, I introduced two reusable ADF pipelines for data-warehouse loads:
    </p>
    <ul>
      <li><strong>PL_Warehouse_Overwrite:</strong> truncates and inserts into the target DW table.</li>
      <li><strong>PL_Warehouse_Merge:</strong> stages data into an ETL table, then dynamically builds and executes a SQL MERGE statement.</li>
    </ul>
    <p>
      This change also transitioned DW loading from Databricks JDBC/ODBC notebooks to native ADF copy activities, improving performance and scalability—especially for large datasets that had previously failed due to memory constraints.
    </p>
  </div>

  <div class="section">
    <h2>Terminology & Documentation Clarity</h2>
    <p>To reduce confusion and improve adoption, I introduced a clear and consistent vocabulary:</p>
    <ul>
      <li><strong>Workflows:</strong> The schedulable unit of execution, composed of jobs.</li>
      <li><strong>Jobs:</strong> Tasks within a workflow, run by pipelines.</li>
      <li><strong>Pipelines:</strong> Reusable components for ingesting, transforming, or loading data.</li>
      <li><strong>Notebooks:</strong> A tool for pipeline implementation, not the orchestration unit.</li>
    </ul>
    <p>
      To support long-term sustainability, I documented the entire framework internally, with detailed reference pages for every database object, notebook, and pipeline. I also onboarded teammates and led training, enabling the framework to scale quickly across use-cases.
    </p>
  </div>
</div>
