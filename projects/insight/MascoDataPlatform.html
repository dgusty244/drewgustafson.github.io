---
layout: default
title: Insight Consulting Project - Azure Data Platform POC for Masco Corporation | Drew Gustafson
---
<div class="container">
  <h1>Insight Consulting - Azure Data Platform Proof of Concept for Masco Corporation</h1>
  <div class="section">
    <p>
      As the sole consultant on this greenfield engagement, I delivered an
      end-to-end Azure data platform proof of concept (POC) that showcased ingestion,
      transformation, modeling, and visualization capabilities for Masco Corporation.
    </p>
  </div>

  <!-- Architecture Diagram -->
  <div class="section">
    <h2>Architecture Overview</h2>
    <div style="text-align: center; margin: 20px 0;">
      <img src="{{ site.baseurl }}/images/azurePocDiagram.png" 
           alt="Azure Data Platform Architecture Diagram" 
           style="max-width: 100%; height: auto; border: 1px solid #ddd; border-radius: 5px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
    </div>
    <p style="text-align: center; font-style: italic; color: #666; font-size: 0.9em;">
      End-to-end Azure data platform architecture provisioned via Terraform IaC
    </p>
  </div>

  <!-- Infrastructure Provisioning -->
  <div class="section">
    <h2>Infrastructure Provisioning</h2>
    <p>
      I customized and executed a Terraform script, my first client-facing IaC deployment, to stand
      up the core Azure resources:
    </p>
    <ul>
      <li>Azure Data Factory</li>
      <li>Azure Databricks</li>
      <li>Azure Data Lake Storage (ADLS)</li>
      <li>Azure Key Vault</li>
    </ul>
    <p>
      Key responsibilities included naming conventions, resource grouping, and identity
      configuration (service principals, app registrations) to ensure best-practice security and
      manageability.
    </p>
  </div>
  <!-- Data Modeling -->
  <div class="section">
    <h2>Data Modeling from Raw CSVs</h2>
    <p>
      The client provided five large CSV files with identical schemas but covering different date ranges. My goal was to design a Kimball-style dimensional model consisting of a sales fact table and dimension tables for products, customers, locations, and subsidiaries.
    </p>
    <p>
      I conducted exploratory analysis to determine the appropriate grain and to identify potential dimensions by selecting distinct values from key attributes—such as company name, store ID, and region—and validating with stakeholders which entities represented subsidiaries or store-level operations.
    </p>
    <p>
      I also cleaned and standardized user-input columns like addresses, mapping variations (e.g., "123 Main St." vs. "123 Main Street") to consistent values. These assumptions were validated through multiple review sessions with the client to ensure alignment between business understanding and technical implementation.
    </p>
  </div>
  <!-- External Data Integration -->
  <div class="section">
    <h2>Enhancements &amp; External Data Integration</h2>
    <p>
      Finishing core scope ahead of schedule, I integrated <em>daily copper prices</em> via a
      public API—critical for a manufacturer reliant on copper. A Databricks notebook ingested the
      feed into a Delta Lake table, enabling time-series joins with sales data and richer insight
      in downstream reports.
    </p>
  </div>
  <!-- Power BI Dashboard -->
  <div class="section">
    <h2>Power BI Demo Dashboard</h2>
    <p>
      I built a demonstration Power BI report featuring:
    </p>
    <ul>
      <li>Sales trends over time</li>
      <li>Sales by hierarchical product classification</li>
      <li>Performance comparisons between child companies</li>
    </ul>
    <p>
      The dashboard served to illustrate platform potential; Masco stakeholders were extremely
      pleased with the technical execution and business value of the POC.
    </p>
  </div>
</div>
